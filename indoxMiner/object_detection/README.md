Below is an updated version of the README with citations and reference links for all mentioned models.

---

# IndoxMiner: Object Detection Models

This repository includes multiple object detection models integrated into the IndoxMiner framework. Each model may have its own license, terms of use, and citation requirements. Please consult the individual model's repository or paper for more details.

## Model Citations

### 1. **Kosmos-2**
- **Paper**: [Kosmos-2: Grounding Multimodal Large Language Models to the World](https://arxiv.org/abs/2306.14824)
- **Citation**:
  ```bibtex
  @misc{peng2023kosmos2,
    title={Kosmos-2: Grounding Multimodal Large Language Models to the World},
    author={Peng, Baolin and Wang, Chenfei and Li, Junyang and Yuan, Hangbo and Zhang, Jing and Chang, Kai-Wei and Gao, Jianfeng and Wang, Yongcheng and others},
    year={2023},
    eprint={2306.14824},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
  }
  ```

---

### 2. **RT-DETR**
- **Paper**: [RT-DETR: Real-time Detector with Transformer](https://arxiv.org/abs/2304.08069)
- **Citation**:
  ```bibtex
  @article{lv2023rtdetr,
    title={RT-DETR: Real-time Detection Transformer with LSH-based Dynamic Anchor Boxes},
    author={Lv, Tiejian and Zhang, Wenwei and Zhao, Zeming and Li, Chao and Sun, Jian and others},
    journal={arXiv preprint arXiv:2304.08069},
    year={2023}
  }
  ```

---

### 3. **LLaVA-NeXT**
- **Papers & Citations**:
  - LLaVA-NeXT:
    ```bibtex
    @misc{liu2024llavanext,
         title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
         url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
         author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
         month={January},
         year={2024}
    }
    ```
  - Improved Baselines:
    ```bibtex
    @misc{liu2023improvedllava,
         title={Improved Baselines with Visual Instruction Tuning}, 
         author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
         eprint={2310.03744},
         archivePrefix={arXiv},
         primaryClass={cs.CL},
         year={2023},
    }
    ```
  - Visual Instruction Tuning:
    ```bibtex
    @misc{liu2023llava,
         title={Visual Instruction Tuning}, 
         author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
         booktitle = {NeurIPS 2023},
         year={2023},
    }
    ```

- **Links**:
  - [LLaVA-NeXT Announcement](https://llava-vl.github.io/blog/2024-01-30-llava-next/)
  - [Improved Baselines Paper](https://arxiv.org/abs/2310.03744)
  - [Visual Instruction Tuning (NeurIPS 2023)](https://proceedings.neurips.cc/paper/2023)

---

### 4. **GroundingDINO**
- **Paper**: [Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://arxiv.org/abs/2303.05499)
- **Citation**:
  ```bibtex
  @article{liu2023grounding,
    title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},
    author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and others},
    journal={arXiv preprint arXiv:2303.05499},
    year={2023}
  }
  ```

---

### 5. **YOLOX**
- **Paper**: [YOLOX: Exceeding YOLO Series in 2021](https://arxiv.org/abs/2107.08430)
- **Citation**:
  ```bibtex
  @article{yolox2021,
    title={YOLOX: Exceeding YOLO Series in 2021},
    author={Ge, Zheng and Liu, Songtao and Wang, Feng and Li, Zeming and Sun, Jian},
    journal={arXiv preprint arXiv:2107.08430},
    year={2021}
  }
  ```

---

### 6. **OWL-ViT**
- **Paper**: [Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2112.01526)
- **Citation**:
  ```bibtex
  @article{minderer2022simple,
    title={Simple Open-Vocabulary Object Detection with Vision Transformers},
    author={Minderer, Matthias and Weninger, Markus and Zhai, Xiaohua and Fayyaz, Mohsen and Ilharco, Gabriel and
            Yang, Kevin and Mottaghi, Roozbeh and Tay, Yi and Dehghani, Mostafa and Vinyals, Oriol and others},
    journal={arXiv preprint arXiv:2112.01526},
    year={2022}
  }
  ```

---

### 8. **Detectron2**
- **Repository & Documentation**: [Detectron2](https://github.com/facebookresearch/detectron2)
- **Citation**:
  ```bibtex
  @misc{wu2019detectron2,
    author =       {Yuxin Wu and Alexander Kirillov and Francisco Massa and Wan-Yen Lo and Ross Girshick},
    title =        {Detectron2},
    howpublished = {\url{https://github.com/facebookresearch/detectron2}},
    year =         {2019}
  }
  ```

---

### 9. **SAM2 (Segment Anything Model v2)**
SAM2 is based on the [Segment Anything](https://arxiv.org/abs/2304.02643) framework:
- **Paper**: [Segment Anything](https://arxiv.org/abs/2304.02643)
- **Citation**:
  ```bibtex
  @article{kirillov2023segment,
    title={Segment Anything},
    author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Vedaldi, Andrea and Doll{\'a}r, Piotr and Girshick, Ross},
    journal={arXiv preprint arXiv:2304.02643},
    year={2023}
  }
  ```

